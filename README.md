# Mobile Robot AI

### Introduction
An introduction to this repository.<br>
[Go to Introduction](#intro)

### How it works
A rapid description of how the program works (pseudo-code).<br>
[Go to How it works](#how)

### Prerequisites
Needed prerequisites to correctly install.<br>
[Go to Prerequisites](#pre)

### Installation
How to install.<br>
[Go to Installation](#install)

### Configuration
How to configure.<br>
[Go to Configuration](#config)

### Execution
How to execute.<br>
[Go to Execution](#exec)

### Improvements
Possible improvements.<br>
[Go to Improvements](#improve)

<a name="intro"></a>
# Introduction

This repository, built on ROS2, integrates the Nav2 stack for autonomous navigation and combines it with control inputs generated by a manually coded PID controller and the OpenAI API. <br>
The project aims to explore whether Large Language Models (which are typically not designed for direct calculations and instead return the most probable result) can effectively generate these control inputs.

<a name="how"></a>
# How it works

The program subscribes to goal pose, laser scanner, and path topics. The path is then modified by altering a user-defined percentage of points and republished on a different topic. This modification reduces the path's complexity, which is necessary because making requests through the API takes approximately 2 seconds. <br>
Then, the control inputs are computed using a PID controller, either by performing direct calculations or by allowing the AI to generate them.

<a name="pre"></a>
# Prerequisites
In order to use the API you need to <a href="https://platform.openai.com/docs/quickstart">create and set the API key</a>.<br>
In addition, the following packages must be installed:
<ul>
    <li><a href="https://docs.nav2.org/getting_started/index.html">navigation2</a></li>
    <li><a href="https://docs.nav2.org/getting_started/index.html">nav2-bringup</a></li>
    <li><a href="https://docs.nav2.org/getting_started/index.html">turtlebot3-gazebo</a></li>
</ul>
You can easily install all of them by running the <i><b>config.sh</b></i> file, passing the ROS2 version name as argument.

<a name="install"></a>
# Installation 

<ol>
    <li>Go into the src folder of your ROS 2 workspace.<br></li> 
    <li>Download the repository:
    <pre><code>git clone https://github.com/simone-contorno/mobile_robot_ai</code></pre>
    </li>
    <li>Go into the root folder of your ROS 2 workspace and build it: 
    <pre><code>colcon build --packages-select mobile_robot_ai</code></pre>
    </li>
</ol>

<a name="config"></a>
# Configuration
The program can be configured using the <i><b>control_config.txt</b></i> file by:
<ul>
    <li>Tuning the PID gains.</li>
    <li>Change the goal threashold.</li>
    <li>Choosing the control mode:
    <ul>
        <li><b>0</b> for manual PID control.</li>
        <li><b>1</b> for OpenAI API-generated PID control.</li></ul>
    </li>
    <li>Setting the AI model and the system prompt.</li>
</ul>
The system prompts are specified in the <i><b>mobile_robot_ai/ai_prompts.py</b></i> file.<br>

<a name="exec"></a>
# Execution

To run the environment simulation you first need to configure it:
<pre><code>
export TURTLEBOT3_MODEL=waffle ;
export GAZEBO_MODEL_PATH=$GAZEBO_MODEL_PATH:$(ros2 pkg prefix mobile_robot_ai)/share/mobile_robot_ai/worlds/ 
</code></pre>

Then, you can launch it:
<pre><code>
ros2 launch mobile_robot_ai simulation_launch.py cmd_vel_remap:=/mobile_robot_ai/cmd_vel plan_remap:=/mobile_robot_ai/plan
</code></pre>
Here the remap is necessary to consider the modified plan by <bi>data_subscriber.cpp</bi> node.

Finally, you can run the following nodes:
<pre><code>
ros2 run mobile_robot_ai data_subscriber ; 
ros2 run mobile_robot_ai pid_control.py 
</code></pre>

To easily execute all this commands, run the <i><b>run.py</b></i> file.

<a name="improve"></a>
# Improvements

Possible future improvements include:
<ul>
    <li>Enhancing the logic of the prompts and rewriting them in XML format.</li>
    <li>If the necessary resources are available, attempting to generate control inputs using a locally installed Large Language Model (LLM).</li> 
</ul>
